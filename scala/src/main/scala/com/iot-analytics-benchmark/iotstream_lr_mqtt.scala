/*
iotstream_lr_mqtt.scala: Spark streaming program to analyze data generated by sim_sensors_lr_mqtt.scala with Logistic Regression using MQTT input
Outputs prediction of model based on input data
  
In one window:
  $ scala -cp <path>iotstream_<scala version>-<code version>.jar:<path>org.eclipse.paho.client.mqttv3-1.2.0.jar com.iotstream.sim_sensors_lr_mqtt n_sensors \
    average_sensor_events_per_second total_events mqtt_server mqtt_port mqtt_topic
  
In another window:
  $ spark-submit --name iotstream_lr_mqtt --class com.iotstream.iotstream_lr_mqtt <path>iotstream_<scala version>-<code version>.jar n_sensors reporting_interval \
    mqtt_server mqtt_topic HDFS_or_S3 HDFS_path_or_S3_bucket modelname

Copyright (c) 2018 VMware, Inc.

This product is licensed to you under the Apache 2.0 license (the "License").  You may not use this product except in compliance with the Apache 2.0 License.

This product may include a number of subcomponents with separate copyright notices and license terms. Your use of these subcomponents is subject to the terms and conditions of the subcomponent's license, as noted in the LICENSE file.

*/

package com.iotstream

import org.apache.spark._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming._
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.mqtt._
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.SparkContext._
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.classification.LogisticRegressionModel
import java.time._

object iotstream_lr_mqtt {
  def main(args: Array[String]) {
  
    if (args.length != 8) {
      System.err.println("Usage: spark-submit --name iotstream_lr_mqtt --class com.iotstream.iotstream_lr_mqtt <path>iotstream_<scala version>-<code version>.jar n_sensors reporting_interval mqtt_server mqtt_port mqtt_topic HDFS_or_S3 HDFS_path_or_S3_bucket modelname")
      System.exit(-1)
    }
  
    val n_sensors = args(0).toInt
    val reporting_interval = args(1).toLong
    val mqtt_URL = "tcp://%s:%s".format(args(2), args(3))
    val mqtt_topic = args(4)

    val modelname = {
      if (args(5).capitalize == "S3") "s3a://%s/%s".format(args(6), args(7))
      else "%s/%s".format(args(6), args(7))
    }
  
    println("%s: Analyzing stream of input from MQTT topic %s with MQTT URL %s, using LR model %s, with %d second intervals".format(Instant.now.toString, mqtt_topic, mqtt_URL, modelname, reporting_interval))

    // Initialize streaming for specified reporting interval
    val conf = new SparkConf().setAppName("iotstream_lr_mqtt")
    val sc = new SparkContext(conf)
    val interval = sc.accumulator(0)
    val empty_intervals = sc.accumulator(0)
    val events  = sc.accumulator(0)
    val ssc = new StreamingContext(sc, Seconds(reporting_interval))
    val sensor_stream = MQTTUtils.createStream(ssc, mqtt_URL, mqtt_topic, StorageLevel.MEMORY_ONLY_SER_2)
    // Load pre-computed model
    val model = LogisticRegressionModel.load(sc, modelname)

    //  Initialize features to <number of sensors>-length array, filled with neutral initial sensor value
    val features = Array.fill(n_sensors)(0.5)

    def run_model(rdd: RDD[String]): Unit = {
      var last_batch = false
      // Input rdd consists of lines of sensor inputs received during that particular batch
      // This code combines the lines in each batch into a feature vector containing the latest sensor values
      // Don't process empty batches
      if (rdd.count == 0) {
        empty_intervals.add(1)
        println("No input")
      } 
      else {
        // Each line of input has format Timestamp (string), sensor number (integer), sensor name (string), sensor value (float), eg
        // 2017-12-14T22:22:43.895Z,19,Sensor 19,0.947640
        // Split each line into its fields, filter out any lines that are not the expected length, then
        //  create a list of tuples with each tuple representing (sensor number, sensor value)
        val input = rdd.map(_.split(",")).filter(_.length == 4).map(list => (list(1).toInt, list(3).toFloat)).collect
        // Read input into features vector. If a sensor was not read during this interval its current value will persist. A negative sensor value means end of events.
        for (t <- input) {
          if (t._1 < 0) last_batch = true
          else features(t._1-1) = t._2
        }
        // If model predicts True warn user in red
        if (model.predict(Vectors.dense(features)) > 0) {
          println("\033[31m%s: Interval %d: Attention needed (%d sensor events in interval)\033[0m".format(Instant.now.toString, interval.value, input.length))
        } else {
          println("%s: Interval %d: Everything is OK (%d sensor events in interval)".format(Instant.now.toString, interval.value, input.length))
        }
        interval.add(1)
        events.add(input.length)
      }
      if (last_batch) ssc.stop()
    }

    // Run model on each batch
//  sensor_stream.print()
    // Discretized stream consists of (offset, RDD) tuples; discard offset
//  sensor_stream.map(_._2).foreachRDD(run_model(_))
    sensor_stream.foreachRDD(run_model(_))
     
    // Start reading streaming data
    ssc.start()
    val start_time = System.nanoTime
    ssc.awaitTermination()
    val finish_time = System.nanoTime
    val elapsed_time = (finish_time - start_time)/1000000000.0  - empty_intervals.value*reporting_interval - 1  // Subtract off time waiting for events and 1 sec for termination
    println("\n%s: %d events received in %.1f seconds (%d intervals), or %.0f sensor events/second\n".format(Instant.now.toString, events.value-1, elapsed_time, interval.value, (events.value-1).toFloat/elapsed_time))
  }
}
